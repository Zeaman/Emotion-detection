{"cells":[{"cell_type":"markdown","metadata":{"id":"02de6873"},"source":["## Testing the trained data both using video feed and live demo video"]},{"cell_type":"markdown","metadata":{"id":"2a6f3322"},"source":["### Step-1: Import important libraries"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":598,"status":"ok","timestamp":1663954464785,"user":{"displayName":"Amanuel Mihiret","userId":"13789695337365511114"},"user_tz":-180},"id":"c8dd5a56"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from keras.models import model_from_json\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"markdown","metadata":{"id":"86a99c86"},"source":["### step-2: Give a lable for catagories"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":643,"status":"ok","timestamp":1663950352529,"user":{"displayName":"Amanuel Mihiret","userId":"13789695337365511114"},"user_tz":-180},"id":"094c2fcb"},"outputs":[],"source":["emotion_dict = {0: \"Facial Tension\", 1: \"No Facial Tension\"}"]},{"cell_type":"markdown","metadata":{"id":"1de7c05c"},"source":["### Step-3: load json file and create model"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3305,"status":"ok","timestamp":1663950359514,"user":{"displayName":"Amanuel Mihiret","userId":"13789695337365511114"},"user_tz":-180},"id":"a9815fe3"},"outputs":[],"source":["json_file = open('/content/best_model/emotion_model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","emotion_model = model_from_json(loaded_model_json)"]},{"cell_type":"markdown","metadata":{"id":"f59e5305"},"source":["### Step-4: load weights (.h5 file) into new model"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":623,"status":"ok","timestamp":1663950363750,"user":{"displayName":"Amanuel Mihiret","userId":"13789695337365511114"},"user_tz":-180},"id":"20459a62","outputId":"0466567f-973f-476f-b1f5-0e560878f721"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded model from disk\n"]}],"source":["emotion_model.load_weights(\"/content/best_model/emotion_model.h5\")\n","print(\"Loaded model from disk\")"]},{"cell_type":"markdown","metadata":{"id":"05e58d79"},"source":["### Step-5: Start the webcam feed the live video recording"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":612,"status":"ok","timestamp":1663952537152,"user":{"displayName":"Amanuel Mihiret","userId":"13789695337365511114"},"user_tz":-180},"id":"3cadaee3"},"outputs":[],"source":["#for live webcam feed: test-1\n","cap = cv2.VideoCapture(0)"]},{"cell_type":"markdown","metadata":{"id":"62886234"},"source":["### Step-6: Use custom feed video"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":714,"status":"ok","timestamp":1663953848644,"user":{"displayName":"Amanuel Mihiret","userId":"13789695337365511114"},"user_tz":-180},"id":"dcd8c635"},"outputs":[],"source":["#for video path feed: test-2\n","cap = cv2.VideoCapture(\"/content/sample_video/sample1.mp4\")"]},{"cell_type":"markdown","metadata":{"id":"ff64498b"},"source":["### Step-7: Start testing by processing the given video using Haar-algorithm for face detection"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OnX9I-NnOSqPYuLg1nJFanNCkoCxJUKH"},"collapsed":true,"id":"debc3d77","outputId":"db1ce7fd-1f4c-4328-bce1-9c8e843eacc4"},"outputs":[],"source":["while True:\n","    # Find haar cascade to draw bounding box around face\n","    ret, frame = cap.read()\n","    frame = cv2.resize(frame, (1280, 720))\n","    if not ret:\n","        break\n","    face_detector = cv2.CascadeClassifier('/content/haardcascade/haarcascade_frontalface_default.xml')\n","    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # detect faces available on camera\n","    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n","\n","    # take each face available on the camera and Preprocess it using a rectangle with gree color\n","    for (x, y, w, h) in num_faces:\n","        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n","        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n","        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n","\n","        # predict the emotions\n","        emotion_prediction = emotion_model.predict(cropped_img)\n","        maxindex = int(np.argmax(emotion_prediction))\n","        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n","   \n","    #cv2.imshow('Emotion Detection', frame)\n","    cv2_imshow(frame)\n","    if cv2.waitKey(1) \u0026 0xFF == ord('q'):       # Hit letter q to exit from the image capture\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"","version":""},"kernelspec":{"display_name":"firstJob","language":"python","name":"firstjob"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}